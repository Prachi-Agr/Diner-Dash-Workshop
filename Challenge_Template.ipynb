{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diner_Dash_Challenge_Template.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jhLb7POHP-Ic",
        "ul2Fi9PaPgz7",
        "_ogJEf9DnYzs",
        "z-SxaBaqp7be"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AISG-Technology-Team/Diner-Dash-Workshop/blob/master/Challenge_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1uhFSLspLGu",
        "colab_type": "text"
      },
      "source": [
        "# **Diner Dash Challenge**\n",
        "\n",
        "---\n",
        "\n",
        "## Objective:\n",
        "\n",
        "Using Reinforcement Learning(RL) algorithms, maximise the average rewards from 100 games/episodes of Diner Dash.\n",
        "\n",
        "## Instructions and Expectations:\n",
        "\n",
        "1. Please use Google Colabs for all computing needs (installing of dependencies, training of model, testing of model, generation of submission, etc). This is to ensure fairness in this competition. You can run multiple notebooks but please take note of the contraints of GPU usage.\n",
        "\n",
        "2. You are required to submit this google colab notebook as well as action lists (in .json format) for each seeded environment given. Hence, please do not change the code under the \"Testing of policies and verification of submission\" apart from that indicated in the chanageable area. For more information about the submission, please refer to the [workshop repo](https://github.com/AISG-Technology-Team/Diner-Dash-Workshop).\n",
        "\n",
        "3. Please update the group member names as well as names of algorithms used in the \"Details of Submission\" section\n",
        "\n",
        "4. We expect to see that the models are learning during training.\n",
        "\n",
        "5. If you have any questions, please discuss within your groups first. Otherwise, please check if the issue is existing on the [workshop repo](https://github.com/AISG-Technology-Team/Diner-Dash-Workshop/issues) or raise one if it is not.\n",
        "\n",
        "## Advice on approach to challenge\n",
        "\n",
        "1. Spend some time to read up about the various RL algos, especially easily implementable baselines\n",
        "\n",
        "2. Split the shortlisted algos among the group\n",
        "\n",
        "3. If necessary, tune the hyperparameters to ensure learning\n",
        "\n",
        "4. Have fun!\n",
        "\n",
        "## Important Resources:\n",
        "\n",
        "1. [Diner Dash repo](https://github.com/AdaCompNUS/diner-dash-simulator)\n",
        "\n",
        "2. [Workshop repo](https://github.com/AISG-Technology-Team/Diner-Dash-Workshop)\n",
        "\n",
        "3. [Stable Baselines](https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "## Things to note:\n",
        "\n",
        "1. Please change the runtime to a GPU when using a GPU. In the above tabs, click Runtime > Change runtime type > GPU in the Hardware accelerator dropdown.\n",
        "\n",
        "2. If an \"Error: A module (diner_dash) was specified for the environment but was not found, make sure the package is installed with `pip install` before calling `gym.make()`\" error is raised, please restart the runtime and rerun the installation of the diner dash simulator.\n",
        "\n",
        "2. Please ensure a strong internet connection throughout this challenge to avoid disconnecting from the collab GPUs\n",
        "\n",
        "3. Do not idle your computer as collab automatically disconnects GPUs if the idle time is too long\n",
        "\n",
        "4. GPUs run on CUDA 10.1\n",
        "\n",
        "For other FAQs, refer to this [link](https://research.google.com/colaboratory/faq.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLMQrYZNhc12",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLhRQoYbhGvh",
        "colab_type": "text"
      },
      "source": [
        "# Details of Submission [Please Edit]\n",
        "\n",
        "### Names of Group Members:\n",
        "John, Mary, Bryan\n",
        "\n",
        "### Names of Algorithms Used:\n",
        "Random Agent, PPO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Sr4eQLrZ3f",
        "colab_type": "text"
      },
      "source": [
        "# Python Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwHyupUqn9I0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2c91bb4-c9ec-47ab-c837-5e5d6b310065"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGTjEEIFFH5u",
        "colab_type": "text"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "To store trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1zPeIVOE2c6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9095e2ea-9ca6-4743-ff17-e3f2151a018c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb_lw5zhJ6Av",
        "colab_type": "text"
      },
      "source": [
        "## Create Project Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4uwe83-Fo0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5f9dd51-81a5-49a6-9ad2-545434092e85"
      },
      "source": [
        "from os import path, chdir, getcwd, mkdir\n",
        "\n",
        "# Choose a project name\n",
        "projectName = \"DinerDashChallenge\"\n",
        "\n",
        "# Project directory is in My Drive\n",
        "projectDirectory = \"/content/drive/My Drive/\" + projectName\n",
        "\n",
        "# Checks if cwd is in content folder\n",
        "if getcwd() == \"/content\":\n",
        "  # Makes project directory if it does not exist\n",
        "  if not path.isdir(projectDirectory):\n",
        "    mkdir(projectDirectory)\n",
        "    print(f\"Project {projectName} has been created!\")\n",
        "  # Changes to project directory\n",
        "  chdir(projectDirectory)\n",
        "\n",
        "print(f\"The current working directory is {getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The current working directory is /content/drive/My Drive/DinerDashChallenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhJmt90nhfuz",
        "colab_type": "text"
      },
      "source": [
        "# Installing Dependencies\n",
        "\n",
        "Downloading relevant project dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iYXlsBohnCo",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies for [diner dash simulator](https://github.com/AdaCompNUS/diner-dash-simulator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h24WNb92iLo5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "418ec08c-3a7d-44d8-9d30-eb330025b64f"
      },
      "source": [
        "from os import path, getcwd\n",
        "\n",
        "repoName = \"diner-dash-simulator\"\n",
        "\n",
        "# Clones repo if it does not exist\n",
        "if not path.isdir(repoName):\n",
        "  !git clone https://github.com/AdaCompNUS/diner-dash-simulator.git\n",
        "  print(f\"Diner Dash repo has been cloned to {getcwd()}\")\n",
        "else:\n",
        "  print(f\"Diner Dash repo is already available at {path.join(getcwd(), repoName)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Diner Dash repo is already available at /content/drive/My Drive/DinerDashChallenge/diner-dash-simulator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArJDnBT3eq_g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "98c56005-29d7-4215-dd73-ba30c0bbef69"
      },
      "source": [
        "!pip install -e diner-dash-simulator/DinerDashEnv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/My%20Drive/DinerDashChallenge/diner-dash-simulator/DinerDashEnv\n",
            "Requirement already satisfied: gym>=0.2.3 in /usr/local/lib/python3.6/dist-packages (from diner-dash==0.0.1) (0.17.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from diner-dash==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from diner-dash==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.2.3->diner-dash==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.2.3->diner-dash==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.2.3->diner-dash==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->diner-dash==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->diner-dash==0.0.1) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->diner-dash==0.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->diner-dash==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.2.3->diner-dash==0.0.1) (0.16.0)\n",
            "Installing collected packages: diner-dash\n",
            "  Found existing installation: diner-dash 0.0.1\n",
            "    Can't uninstall 'diner-dash'. No files were found to uninstall.\n",
            "  Running setup.py develop for diner-dash\n",
            "Successfully installed diner-dash\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFG_CNITTHDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5eadca2e-7562-4635-b6fd-451383708966"
      },
      "source": [
        "import gym\n",
        "\n",
        "# Test make environment\n",
        "def testEnv():\n",
        "  env = gym.make('diner_dash:DinerDash-v0').unwrapped\n",
        "  env.flash_sim = False\n",
        "  env.close()\n",
        "  return True\n",
        "\n",
        "if testEnv():\n",
        "  print(\"Installation of diner dash simulator is successful!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installation of diner dash simulator is successful!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI9AG8cflhV2",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies for Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZQV-Quxmd_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b660c35e-d831-4450-9a3d-ccf89d0befb6"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.5.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.6.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3vS8KYfpIan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "c484dffc-29b3-45ef-b0e9-d430ec09df1e"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/fe/db8159d4d79109c6c8942abe77c7ba6b6e008c32ae55870a35e73fa10db3/stable_baselines-2.10.0-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.18.5)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.12.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhLb7POHP-Ic",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions\n",
        "\n",
        "For easier debugging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMDW4nVXQA4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAction(actionID):\n",
        "    actionIDtoName = {\n",
        "        0 : \"None\",\n",
        "        1 : \"Move to Table 1\",\n",
        "        2 : \"Move to Table 2\",\n",
        "        3 : \"Move to Table 3\",\n",
        "        4 : \"Move to Table 4\",\n",
        "        5 : \"Move to Table 5\",\n",
        "        6 : \"Move to Table 6\",\n",
        "        7 : \"Move to Counter\",\n",
        "        8 : \"Pick Food for Table 1\",\n",
        "        9 : \"Pick Food for Table 2\",\n",
        "        10 : \"Pick Food for Table 3\",\n",
        "        11 : \"Pick Food for Table 4\",\n",
        "        12 : \"Pick Food for Table 5\",\n",
        "        13 : \"Pick Food for Table 6\",\n",
        "        14 : \"Move to Food Collection\",\n",
        "        15 : \"Pick Table 1 for Group 1\",\n",
        "        16 : \"Pick Table 2 for Group 1\",\n",
        "        17 : \"Pick Table 3 for Group 1\",\n",
        "        18 : \"Pick Table 4 for Group 1\",\n",
        "        19 : \"Pick Table 5 for Group 1\",\n",
        "        20 : \"Pick Table 6 for Group 1\",\n",
        "        21 : \"Pick Table 1 for Group 2\",\n",
        "        22 : \"Pick Table 2 for Group 2\",\n",
        "        23 : \"Pick Table 3 for Group 2\",\n",
        "        24 : \"Pick Table 4 for Group 2\",\n",
        "        25 : \"Pick Table 5 for Group 2\",\n",
        "        26 : \"Pick Table 6 for Group 2\",\n",
        "        27 : \"Pick Table 1 for Group 3\",\n",
        "        28 : \"Pick Table 2 for Group 3\",\n",
        "        29 : \"Pick Table 3 for Group 3\",\n",
        "        30 : \"Pick Table 4 for Group 3\",\n",
        "        31 : \"Pick Table 5 for Group 3\",\n",
        "        32 : \"Pick Table 6 for Group 3\",\n",
        "        33 : \"Pick Table 1 for Group 4\",\n",
        "        34 : \"Pick Table 2 for Group 4\",\n",
        "        35 : \"Pick Table 3 for Group 4\",\n",
        "        36 : \"Pick Table 4 for Group 4\",\n",
        "        37 : \"Pick Table 5 for Group 4\",\n",
        "        38 : \"Pick Table 6 for Group 4\",\n",
        "        39 : \"Pick Table 1 for Group 5\",\n",
        "        40 : \"Pick Table 2 for Group 5\",\n",
        "        41 : \"Pick Table 3 for Group 5\",\n",
        "        42 : \"Pick Table 4 for Group 5\",\n",
        "        43 : \"Pick Table 5 for Group 5\",\n",
        "        44 : \"Pick Table 6 for Group 5\",\n",
        "        45 : \"Pick Table 1 for Group 6\",\n",
        "        46 : \"Pick Table 2 for Group 6\",\n",
        "        47 : \"Pick Table 3 for Group 6\",\n",
        "        48 : \"Pick Table 4 for Group 6\",\n",
        "        49 : \"Pick Table 5 for Group 6\",\n",
        "        50 : \"Pick Table 6 for Group 6\",\n",
        "        51 : \"Pick Table 1 for Group 7\",\n",
        "        52 : \"Pick Table 2 for Group 7\",\n",
        "        53 : \"Pick Table 3 for Group 7\",\n",
        "        54 : \"Pick Table 4 for Group 7\",\n",
        "        55 : \"Pick Table 5 for Group 7\",\n",
        "        56 : \"Pick Table 6 for Group 7\",\n",
        "    }\n",
        "    return actionIDtoName[actionID]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIfwpn5wnTwk",
        "colab_type": "text"
      },
      "source": [
        "# Policies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN5uGdnlqqmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import gym\n",
        "\n",
        "env = gym.make('diner_dash:DinerDash-v0').unwrapped\n",
        "env.flash_sim = False\n",
        "state = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul2Fi9PaPgz7",
        "colab_type": "text"
      },
      "source": [
        "## Random Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StTisAOnPvoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4cYi3VuPjwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Randomly select an action from the action space\n",
        "def testRA(env, state):\n",
        "    # init variables\n",
        "    done = False\n",
        "    sumReward = 0\n",
        "    actionList = []\n",
        "\n",
        "    while not done:\n",
        "        action = randint(0, 56)\n",
        "        actionList.append(action)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        sumReward += reward\n",
        "\n",
        "    return sumReward, actionList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ogJEf9DnYzs",
        "colab_type": "text"
      },
      "source": [
        "## [PPO](https://github.com/nikhilbarhate99/PPO-PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaLiVAuEnP4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.distributions import Categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOdTXCzYmvBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbJ-Mr5ZoCmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "    \n",
        "    def clear_memory(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # actor\n",
        "        self.action_layer = nn.Sequential(\n",
        "                nn.Linear(state_dim, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, action_dim),\n",
        "                nn.Softmax(dim=-1)\n",
        "                )\n",
        "        \n",
        "        # critic\n",
        "        self.value_layer = nn.Sequential(\n",
        "                nn.Linear(state_dim, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, n_latent_var),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(n_latent_var, 1)\n",
        "                )\n",
        "        \n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def act(self, state, memory):\n",
        "        state = torch.from_numpy(state).float().to(device) \n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        \n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(dist.log_prob(action))\n",
        "        \n",
        "        return action.item()\n",
        "    \n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        \n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        \n",
        "        state_value = self.value_layer(state)\n",
        "        \n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
        "        \n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        \n",
        "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        self.MseLoss = nn.MSELoss()\n",
        "    \n",
        "    def update(self, memory):   \n",
        "        # Monte Carlo estimate of state rewards:\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "        \n",
        "        # Normalizing the rewards:\n",
        "        rewards = torch.tensor(rewards).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "        \n",
        "        # convert list to tensor\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "        \n",
        "        # Optimize policy for K epochs:\n",
        "        for _ in range(self.K_epochs):\n",
        "            # Evaluating old actions and values :\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "            \n",
        "            # Finding the ratio (pi_theta / pi_theta__old):\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "                \n",
        "            # Finding Surrogate Loss:\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "            \n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "        \n",
        "        # Copy new weights into old policy:\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2pRyXLDBtnV",
        "colab_type": "text"
      },
      "source": [
        "### Training PPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb-_zyFloDUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainPPO():\n",
        "    ############## Hyperparameters ##############\n",
        "    env_name = \"diner_dash:DinerDash-v0\"\n",
        "    # creating environment\n",
        "    env = gym.make(env_name).unwrapped\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = 57\n",
        "    render = False\n",
        "    solved_reward = 500         # stop training if avg_reward > solved_reward\n",
        "    log_interval = 1000         # print avg reward in the interval\n",
        "    max_episodes = int(1e8)     # max training episodes\n",
        "    max_timesteps = int(1e8)    # max timesteps in one episode\n",
        "    n_latent_var = 64           # number of variables in hidden layer\n",
        "    update_timestep = 2000      # update policy every n timesteps\n",
        "    lr = 0.002\n",
        "    betas = (0.9, 0.999)\n",
        "    gamma = 0.99                # discount factor\n",
        "    K_epochs = 4                # update policy for K epochs\n",
        "    eps_clip = 0.2              # clip parameter for PPO\n",
        "    random_seed = None          # do NOT train with a random seed\n",
        "    #############################################\n",
        "    \n",
        "    if random_seed:\n",
        "        torch.manual_seed(random_seed)\n",
        "        env.seed(random_seed)\n",
        "    \n",
        "    memory = Memory()\n",
        "    ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
        "    print(f\"Device used: {device}\")\n",
        "    print(f\"Learning rate: {lr}, Betas: {betas}\")\n",
        "    \n",
        "    # logging variables\n",
        "    running_reward = 0\n",
        "    avg_length = 0\n",
        "    timestep = 0\n",
        "    \n",
        "    # training loop\n",
        "    for i_episode in range(1, max_episodes+1):\n",
        "        state = env.reset()\n",
        "        state = np.array(state)\n",
        "        \n",
        "        for t in range(max_timesteps):\n",
        "          timestep += 1\n",
        "\n",
        "          # Running policy_old:\n",
        "          action = ppo.policy_old.act(state, memory)\n",
        "          state, reward, done, _ = env.step(action)\n",
        "          state = np.array(state)\n",
        "          \n",
        "          # Saving reward and is_terminal:\n",
        "          memory.rewards.append(reward)\n",
        "          memory.is_terminals.append(done)\n",
        "          \n",
        "          # update if its time\n",
        "          if timestep % update_timestep == 0:\n",
        "              ppo.update(memory)\n",
        "              memory.clear_memory()\n",
        "              timestep = 0\n",
        "          \n",
        "          running_reward += reward\n",
        "          if render:\n",
        "              env.render()\n",
        "          if done:\n",
        "              break\n",
        "                \n",
        "        avg_length += t\n",
        "        \n",
        "        # stop training if avg_reward > solved_reward\n",
        "        if running_reward > (log_interval*solved_reward):\n",
        "            print(\"########## Solved! ##########\")\n",
        "            torch.save(ppo.policy.state_dict(), 'PPO_{}_{}.pth'.format(env_name, solved_reward))\n",
        "            break\n",
        "            \n",
        "        # logging\n",
        "        if i_episode % log_interval == 0:\n",
        "            avg_length = int(avg_length/log_interval)\n",
        "            running_reward = int((running_reward/log_interval))\n",
        "            \n",
        "            print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
        "            running_reward = 0\n",
        "            avg_length = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c06KtkyZLHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "a6e63cb3-2069-4fff-f028-6a7256ea96f7"
      },
      "source": [
        "trainPPO()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used: cuda:0\n",
            "Learning rate: 0.002, Betas: (0.9, 0.999)\n",
            "Device used: cuda:0\n",
            "Learning rate: 0.002, Betas: (0.9, 0.999)\n",
            "Episode 1000 \t avg length: 137 \t reward: -960\n",
            "Episode 1000 \t avg length: 137 \t reward: -960\n",
            "Episode 2000 \t avg length: 137 \t reward: -369\n",
            "Episode 2000 \t avg length: 137 \t reward: -369\n",
            "Episode 3000 \t avg length: 139 \t reward: 50\n",
            "Episode 3000 \t avg length: 139 \t reward: 50\n",
            "Episode 4000 \t avg length: 141 \t reward: 163\n",
            "Episode 4000 \t avg length: 141 \t reward: 163\n",
            "Episode 5000 \t avg length: 141 \t reward: 249\n",
            "Episode 5000 \t avg length: 141 \t reward: 249\n",
            "Episode 6000 \t avg length: 143 \t reward: 318\n",
            "Episode 6000 \t avg length: 143 \t reward: 318\n",
            "Episode 7000 \t avg length: 141 \t reward: 267\n",
            "Episode 7000 \t avg length: 141 \t reward: 267\n",
            "Episode 8000 \t avg length: 142 \t reward: 235\n",
            "Episode 8000 \t avg length: 142 \t reward: 235\n",
            "Episode 9000 \t avg length: 142 \t reward: 273\n",
            "Episode 9000 \t avg length: 142 \t reward: 273\n",
            "Episode 10000 \t avg length: 142 \t reward: 310\n",
            "Episode 10000 \t avg length: 142 \t reward: 310\n",
            "Episode 11000 \t avg length: 145 \t reward: 370\n",
            "Episode 11000 \t avg length: 145 \t reward: 370\n",
            "Episode 12000 \t avg length: 145 \t reward: 417\n",
            "Episode 12000 \t avg length: 145 \t reward: 417\n",
            "Episode 13000 \t avg length: 145 \t reward: 426\n",
            "Episode 13000 \t avg length: 145 \t reward: 426\n",
            "Episode 14000 \t avg length: 147 \t reward: 478\n",
            "Episode 14000 \t avg length: 147 \t reward: 478\n",
            "Episode 15000 \t avg length: 146 \t reward: 477\n",
            "Episode 15000 \t avg length: 146 \t reward: 477\n",
            "Episode 16000 \t avg length: 146 \t reward: 463\n",
            "Episode 16000 \t avg length: 146 \t reward: 463\n",
            "Episode 17000 \t avg length: 146 \t reward: 437\n",
            "Episode 17000 \t avg length: 146 \t reward: 437\n",
            "Episode 18000 \t avg length: 147 \t reward: 494\n",
            "Episode 18000 \t avg length: 147 \t reward: 494\n",
            "Episode 19000 \t avg length: 147 \t reward: 468\n",
            "Episode 19000 \t avg length: 147 \t reward: 468\n",
            "########## Solved! ##########\n",
            "########## Solved! ##########\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Z3eCMVMLx9",
        "colab_type": "text"
      },
      "source": [
        "### Testing PPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUE1qOV9MN3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testPPO(env, state):\n",
        "    ############## Hyperparameters ##############\n",
        "    env_name = \"diner_dash:DinerDash-v0\"\n",
        "    # creating environment\n",
        "    # env = gym.make(env_name).unwrapped\n",
        "    state_dim = 40\n",
        "    action_dim = 57\n",
        "    exp_reward = 500            # expected reward to load saved model     \n",
        "    n_latent_var = 64           # number of variables in hidden layer\n",
        "    lr = 0.0007\n",
        "    betas = (0.9, 0.999)\n",
        "    gamma = 0.99                # discount factor\n",
        "    K_epochs = 4                # update policy for K epochs\n",
        "    eps_clip = 0.2              # clip parameter for PPO\n",
        "    #############################################\n",
        "    \n",
        "    filename = \"PPO_{}_{}.pth\".format(env_name, exp_reward)\n",
        "    \n",
        "    memory = Memory()\n",
        "    ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
        "    \n",
        "    ppo.policy_old.load_state_dict(torch.load(filename))\n",
        "\n",
        "    ep_reward = 0\n",
        "    state = np.array(state)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      action = ppo.policy_old.act(state, memory)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      state = np.array(state)\n",
        "      ep_reward += reward\n",
        "\n",
        "    actionList = [action.item() for action in memory.actions]\n",
        "\n",
        "    return ep_reward, actionList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9AaOurkA68o",
        "colab_type": "text"
      },
      "source": [
        "## Stable Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhlZv0u3Wyn6",
        "colab_type": "text"
      },
      "source": [
        "### Check Env setup for Stable Baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPuA_s1cyeC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "60d36b88-3c18-4956-a01a-1743d363daa5"
      },
      "source": [
        "from stable_baselines.common.env_checker import check_env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkerSMaSym9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9f0cda3-962d-411d-fd56-7e37ea91b30e"
      },
      "source": [
        "error = check_env(gym.make('diner_dash:DinerDash-v0').unwrapped)\n",
        "if error == None:\n",
        "  print(\"Diner Dash environment is compatible with Stable-Baselines!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Diner Dash environment is compatible with Stable-Baselines!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZN2HqxQXU5j",
        "colab_type": "text"
      },
      "source": [
        "### Save or Load Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIQjUYbUXUJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z-SxaBaqp7be"
      },
      "source": [
        "### Vanilla DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhPo3S5eyoAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Same as before we instantiate the agent along with the environment\n",
        "from stable_baselines import DQN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZI9C3utp0g1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2ce27da-83b0-4060-b338-b3628f68e414"
      },
      "source": [
        "# Deactivate all the DQN extensions to have the original version\n",
        "# In practice, it is recommend to have them activated\n",
        "kwargs = {'double_q': False, 'prioritized_replay': False, 'policy_kwargs': dict(dueling=False)}\n",
        "\n",
        "# Initiliase environment\n",
        "env = gym.make('diner_dash:DinerDash-v0').unwrapped\n",
        "env.flash_sim = False\n",
        "\n",
        "# Note that the MlpPolicy of DQN is different from the one of PPO\n",
        "# but stable-baselines handles that automatically if you pass a string\n",
        "dqn_model = DQN('MlpPolicy', env, verbose=1, **kwargs)\n",
        "\n",
        "start_time = time.time()\n",
        "# Train the agent for 10000 steps\n",
        "dqn_model.learn(total_timesteps=int(1e6), log_interval=300)\n",
        "print(f\"--- {(time.time() - start_time)//60} minutes ---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "---------------------------------------\n",
            "| % time spent exploring  | 58        |\n",
            "| episodes                | 300       |\n",
            "| mean 100 episode reward | -1.31e+03 |\n",
            "| steps                   | 41902     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| % time spent exploring  | 18        |\n",
            "| episodes                | 600       |\n",
            "| mean 100 episode reward | -1.28e+03 |\n",
            "| steps                   | 82799     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| % time spent exploring  | 2         |\n",
            "| episodes                | 900       |\n",
            "| mean 100 episode reward | -1.05e+03 |\n",
            "| steps                   | 123282    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1200     |\n",
            "| mean 100 episode reward | -1.1e+03 |\n",
            "| steps                   | 163673   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| % time spent exploring  | 2         |\n",
            "| episodes                | 1500      |\n",
            "| mean 100 episode reward | -1.05e+03 |\n",
            "| steps                   | 203856    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| % time spent exploring  | 2         |\n",
            "| episodes                | 1800      |\n",
            "| mean 100 episode reward | -1.06e+03 |\n",
            "| steps                   | 244638    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2100     |\n",
            "| mean 100 episode reward | -876     |\n",
            "| steps                   | 285250   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2400     |\n",
            "| mean 100 episode reward | -753     |\n",
            "| steps                   | 325398   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 2700     |\n",
            "| mean 100 episode reward | -716     |\n",
            "| steps                   | 366024   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3000     |\n",
            "| mean 100 episode reward | -590     |\n",
            "| steps                   | 406788   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3300     |\n",
            "| mean 100 episode reward | -760     |\n",
            "| steps                   | 447699   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3600     |\n",
            "| mean 100 episode reward | -754     |\n",
            "| steps                   | 488180   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3900     |\n",
            "| mean 100 episode reward | -690     |\n",
            "| steps                   | 528747   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4200     |\n",
            "| mean 100 episode reward | -676     |\n",
            "| steps                   | 569645   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4500     |\n",
            "| mean 100 episode reward | -804     |\n",
            "| steps                   | 610243   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4800     |\n",
            "| mean 100 episode reward | -811     |\n",
            "| steps                   | 650629   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5100     |\n",
            "| mean 100 episode reward | -674     |\n",
            "| steps                   | 692322   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5400     |\n",
            "| mean 100 episode reward | -666     |\n",
            "| steps                   | 734235   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5700     |\n",
            "| mean 100 episode reward | -665     |\n",
            "| steps                   | 776193   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6000     |\n",
            "| mean 100 episode reward | -589     |\n",
            "| steps                   | 818054   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6300     |\n",
            "| mean 100 episode reward | -677     |\n",
            "| steps                   | 860120   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6600     |\n",
            "| mean 100 episode reward | -551     |\n",
            "| steps                   | 902570   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6900     |\n",
            "| mean 100 episode reward | -620     |\n",
            "| steps                   | 944862   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7200     |\n",
            "| mean 100 episode reward | -619     |\n",
            "| steps                   | 986824   |\n",
            "--------------------------------------\n",
            "--- 62.0 minutes ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZcYa0gqva3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e8ca749-4fbc-4c43-f9b2-f5a61141d826"
      },
      "source": [
        "from stable_baselines.common.evaluation import evaluate_policy\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(dqn_model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_reward:-581.60 +/- 469.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWruL1pM4X5H",
        "colab_type": "text"
      },
      "source": [
        "### PPO2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq_Cj8iR0Rqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d25c2041-4b98-4427-c8c7-a2c64e1929c2"
      },
      "source": [
        "from stable_baselines.common import make_vec_env\n",
        "from stable_baselines import PPO2\n",
        "\n",
        "# Vectorize environment\n",
        "env = make_vec_env(\"diner_dash:DinerDash-v0\")\n",
        "\n",
        "model = PPO2('MlpPolicy', env, verbose=1)\n",
        "start_time = time.time()\n",
        "model.learn(total_timesteps=int(5e6), log_interval=300)\n",
        "print(f\"--- Time take to train model = {(time.time() - start_time)//60} minutes ---\")\n",
        "\n",
        "print(\"Saving Model...\")\n",
        "modelDirectory = \"./\"\n",
        "modelName = \"dinerDash\"\n",
        "model.save(modelDirectory + modelName)\n",
        "print(f\"Model saved as {modelDirectory + modelName}\")\n",
        "\n",
        "del model # remove to demonstrate saving and loading"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00016157987 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 120           |\n",
            "| ep_reward_mean     | -1.85e+03     |\n",
            "| explained_variance | -0.00125      |\n",
            "| fps                | 58            |\n",
            "| n_updates          | 1             |\n",
            "| policy_entropy     | 4.042927      |\n",
            "| policy_loss        | -0.00868512   |\n",
            "| serial_timesteps   | 128           |\n",
            "| time_elapsed       | 4.96e-05      |\n",
            "| total_timesteps    | 128           |\n",
            "| value_loss         | 60121.566     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 5.398751e-06  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 139           |\n",
            "| ep_reward_mean     | -1.29e+03     |\n",
            "| explained_variance | 0.0817        |\n",
            "| fps                | 537           |\n",
            "| n_updates          | 300           |\n",
            "| policy_entropy     | 3.9461465     |\n",
            "| policy_loss        | -0.0010429706 |\n",
            "| serial_timesteps   | 38400         |\n",
            "| time_elapsed       | 75.2          |\n",
            "| total_timesteps    | 38400         |\n",
            "| value_loss         | 7338.475      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 4.9867638e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 136           |\n",
            "| ep_reward_mean     | -1.28e+03     |\n",
            "| explained_variance | 0.0827        |\n",
            "| fps                | 504           |\n",
            "| n_updates          | 600           |\n",
            "| policy_entropy     | 3.7404852     |\n",
            "| policy_loss        | -0.0016681274 |\n",
            "| serial_timesteps   | 76800         |\n",
            "| time_elapsed       | 148           |\n",
            "| total_timesteps    | 76800         |\n",
            "| value_loss         | 4915.1445     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 4.15681e-05  |\n",
            "| clipfrac           | 0.0          |\n",
            "| ep_len_mean        | 138          |\n",
            "| ep_reward_mean     | -1.15e+03    |\n",
            "| explained_variance | 0.071        |\n",
            "| fps                | 505          |\n",
            "| n_updates          | 900          |\n",
            "| policy_entropy     | 3.7900321    |\n",
            "| policy_loss        | 0.0007061994 |\n",
            "| serial_timesteps   | 115200       |\n",
            "| time_elapsed       | 221          |\n",
            "| total_timesteps    | 115200       |\n",
            "| value_loss         | 54026.324    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0003209122  |\n",
            "| clipfrac           | 0.001953125   |\n",
            "| ep_len_mean        | 138           |\n",
            "| ep_reward_mean     | -1.02e+03     |\n",
            "| explained_variance | 0.193         |\n",
            "| fps                | 489           |\n",
            "| n_updates          | 1200          |\n",
            "| policy_entropy     | 3.4832268     |\n",
            "| policy_loss        | -0.0066784304 |\n",
            "| serial_timesteps   | 153600        |\n",
            "| time_elapsed       | 294           |\n",
            "| total_timesteps    | 153600        |\n",
            "| value_loss         | 6088.3594     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.9466553e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 139           |\n",
            "| ep_reward_mean     | -1.14e+03     |\n",
            "| explained_variance | 0.287         |\n",
            "| fps                | 539           |\n",
            "| n_updates          | 1500          |\n",
            "| policy_entropy     | 3.4517636     |\n",
            "| policy_loss        | 0.00033317157 |\n",
            "| serial_timesteps   | 192000        |\n",
            "| time_elapsed       | 367           |\n",
            "| total_timesteps    | 192000        |\n",
            "| value_loss         | 38688.668     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.2941331e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 138           |\n",
            "| ep_reward_mean     | -1.14e+03     |\n",
            "| explained_variance | 0.427         |\n",
            "| fps                | 490           |\n",
            "| n_updates          | 1800          |\n",
            "| policy_entropy     | 3.6757672     |\n",
            "| policy_loss        | -0.0008541966 |\n",
            "| serial_timesteps   | 230400        |\n",
            "| time_elapsed       | 440           |\n",
            "| total_timesteps    | 230400        |\n",
            "| value_loss         | 4907.0234     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.1737789e-06 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 138           |\n",
            "| ep_reward_mean     | -1.01e+03     |\n",
            "| explained_variance | 0.475         |\n",
            "| fps                | 559           |\n",
            "| n_updates          | 2100          |\n",
            "| policy_entropy     | 3.4157412     |\n",
            "| policy_loss        | -0.000325124  |\n",
            "| serial_timesteps   | 268800        |\n",
            "| time_elapsed       | 513           |\n",
            "| total_timesteps    | 268800        |\n",
            "| value_loss         | 4629.7427     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0005513221  |\n",
            "| clipfrac           | 0.005859375   |\n",
            "| ep_len_mean        | 140           |\n",
            "| ep_reward_mean     | -906          |\n",
            "| explained_variance | 0.296         |\n",
            "| fps                | 561           |\n",
            "| n_updates          | 2400          |\n",
            "| policy_entropy     | 3.2900076     |\n",
            "| policy_loss        | -0.0045085987 |\n",
            "| serial_timesteps   | 307200        |\n",
            "| time_elapsed       | 585           |\n",
            "| total_timesteps    | 307200        |\n",
            "| value_loss         | 17449.256     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00027571013 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 137           |\n",
            "| ep_reward_mean     | -952          |\n",
            "| explained_variance | 0.76          |\n",
            "| fps                | 523           |\n",
            "| n_updates          | 2700          |\n",
            "| policy_entropy     | 3.4234524     |\n",
            "| policy_loss        | 0.0025820988  |\n",
            "| serial_timesteps   | 345600        |\n",
            "| time_elapsed       | 658           |\n",
            "| total_timesteps    | 345600        |\n",
            "| value_loss         | 1158.6531     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 5.7176903e-06 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 139           |\n",
            "| ep_reward_mean     | -1.03e+03     |\n",
            "| explained_variance | 0.657         |\n",
            "| fps                | 529           |\n",
            "| n_updates          | 3000          |\n",
            "| policy_entropy     | 3.4392557     |\n",
            "| policy_loss        | -0.0009401108 |\n",
            "| serial_timesteps   | 384000        |\n",
            "| time_elapsed       | 731           |\n",
            "| total_timesteps    | 384000        |\n",
            "| value_loss         | 3357.942      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0018625708 |\n",
            "| clipfrac           | 0.0078125    |\n",
            "| ep_len_mean        | 136          |\n",
            "| ep_reward_mean     | -971         |\n",
            "| explained_variance | 0.154        |\n",
            "| fps                | 537          |\n",
            "| n_updates          | 3300         |\n",
            "| policy_entropy     | 3.3991032    |\n",
            "| policy_loss        | -0.011940872 |\n",
            "| serial_timesteps   | 422400       |\n",
            "| time_elapsed       | 804          |\n",
            "| total_timesteps    | 422400       |\n",
            "| value_loss         | 17052.348    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 6.9088237e-06  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 137            |\n",
            "| ep_reward_mean     | -892           |\n",
            "| explained_variance | 0.959          |\n",
            "| fps                | 524            |\n",
            "| n_updates          | 3600           |\n",
            "| policy_entropy     | 3.2861967      |\n",
            "| policy_loss        | -0.00075665547 |\n",
            "| serial_timesteps   | 460800         |\n",
            "| time_elapsed       | 877            |\n",
            "| total_timesteps    | 460800         |\n",
            "| value_loss         | 755.12463      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00072749914 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 138           |\n",
            "| ep_reward_mean     | -936          |\n",
            "| explained_variance | 0.652         |\n",
            "| fps                | 492           |\n",
            "| n_updates          | 3900          |\n",
            "| policy_entropy     | 3.1199422     |\n",
            "| policy_loss        | 0.0045111678  |\n",
            "| serial_timesteps   | 499200        |\n",
            "| time_elapsed       | 950           |\n",
            "| total_timesteps    | 499200        |\n",
            "| value_loss         | 3934.5781     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00060532946  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 135            |\n",
            "| ep_reward_mean     | -894           |\n",
            "| explained_variance | 0.649          |\n",
            "| fps                | 529            |\n",
            "| n_updates          | 4200           |\n",
            "| policy_entropy     | 2.959281       |\n",
            "| policy_loss        | -0.00023645512 |\n",
            "| serial_timesteps   | 537600         |\n",
            "| time_elapsed       | 1.02e+03       |\n",
            "| total_timesteps    | 537600         |\n",
            "| value_loss         | 10918.854      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.2441316e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 136           |\n",
            "| ep_reward_mean     | -709          |\n",
            "| explained_variance | 0.516         |\n",
            "| fps                | 522           |\n",
            "| n_updates          | 4500          |\n",
            "| policy_entropy     | 3.139138      |\n",
            "| policy_loss        | -0.0005898039 |\n",
            "| serial_timesteps   | 576000        |\n",
            "| time_elapsed       | 1.1e+03       |\n",
            "| total_timesteps    | 576000        |\n",
            "| value_loss         | 5532.669      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 2.4110864e-06  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 138            |\n",
            "| ep_reward_mean     | -696           |\n",
            "| explained_variance | -0.374         |\n",
            "| fps                | 513            |\n",
            "| n_updates          | 4800           |\n",
            "| policy_entropy     | 3.2416022      |\n",
            "| policy_loss        | -0.00015276042 |\n",
            "| serial_timesteps   | 614400         |\n",
            "| time_elapsed       | 1.17e+03       |\n",
            "| total_timesteps    | 614400         |\n",
            "| value_loss         | 7038.023       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00024088376 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 136           |\n",
            "| ep_reward_mean     | -643          |\n",
            "| explained_variance | 0.403         |\n",
            "| fps                | 548           |\n",
            "| n_updates          | 5100          |\n",
            "| policy_entropy     | 2.8467398     |\n",
            "| policy_loss        | -0.0029304342 |\n",
            "| serial_timesteps   | 652800        |\n",
            "| time_elapsed       | 1.24e+03      |\n",
            "| total_timesteps    | 652800        |\n",
            "| value_loss         | 5310.9526     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 9.2356946e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 136           |\n",
            "| ep_reward_mean     | -639          |\n",
            "| explained_variance | 0.47          |\n",
            "| fps                | 549           |\n",
            "| n_updates          | 5400          |\n",
            "| policy_entropy     | 2.8956697     |\n",
            "| policy_loss        | 0.00030128856 |\n",
            "| serial_timesteps   | 691200        |\n",
            "| time_elapsed       | 1.31e+03      |\n",
            "| total_timesteps    | 691200        |\n",
            "| value_loss         | 8922.315      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0018087747  |\n",
            "| clipfrac           | 0.009765625   |\n",
            "| ep_len_mean        | 133           |\n",
            "| ep_reward_mean     | -700          |\n",
            "| explained_variance | -0.515        |\n",
            "| fps                | 513           |\n",
            "| n_updates          | 5700          |\n",
            "| policy_entropy     | 3.0868437     |\n",
            "| policy_loss        | -0.0058226073 |\n",
            "| serial_timesteps   | 729600        |\n",
            "| time_elapsed       | 1.39e+03      |\n",
            "| total_timesteps    | 729600        |\n",
            "| value_loss         | 22234.506     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00049622334 |\n",
            "| clipfrac           | 0.00390625    |\n",
            "| ep_len_mean        | 138           |\n",
            "| ep_reward_mean     | -766          |\n",
            "| explained_variance | -1.34         |\n",
            "| fps                | 486           |\n",
            "| n_updates          | 6000          |\n",
            "| policy_entropy     | 3.294375      |\n",
            "| policy_loss        | -0.0067248363 |\n",
            "| serial_timesteps   | 768000        |\n",
            "| time_elapsed       | 1.46e+03      |\n",
            "| total_timesteps    | 768000        |\n",
            "| value_loss         | 8407.688      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00090971356 |\n",
            "| clipfrac           | 0.00390625    |\n",
            "| ep_len_mean        | 137           |\n",
            "| ep_reward_mean     | -570          |\n",
            "| explained_variance | 0.656         |\n",
            "| fps                | 520           |\n",
            "| n_updates          | 6300          |\n",
            "| policy_entropy     | 2.9268289     |\n",
            "| policy_loss        | -0.0060892785 |\n",
            "| serial_timesteps   | 806400        |\n",
            "| time_elapsed       | 1.53e+03      |\n",
            "| total_timesteps    | 806400        |\n",
            "| value_loss         | 13998.615     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0010461346 |\n",
            "| clipfrac           | 0.001953125  |\n",
            "| ep_len_mean        | 136          |\n",
            "| ep_reward_mean     | -293         |\n",
            "| explained_variance | 0.316        |\n",
            "| fps                | 514          |\n",
            "| n_updates          | 6600         |\n",
            "| policy_entropy     | 2.3540118    |\n",
            "| policy_loss        | -0.004607468 |\n",
            "| serial_timesteps   | 844800       |\n",
            "| time_elapsed       | 1.6e+03      |\n",
            "| total_timesteps    | 844800       |\n",
            "| value_loss         | 11669.15     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00041874655 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 135           |\n",
            "| ep_reward_mean     | -270          |\n",
            "| explained_variance | 0.319         |\n",
            "| fps                | 565           |\n",
            "| n_updates          | 6900          |\n",
            "| policy_entropy     | 2.203721      |\n",
            "| policy_loss        | 0.00042217935 |\n",
            "| serial_timesteps   | 883200        |\n",
            "| time_elapsed       | 1.68e+03      |\n",
            "| total_timesteps    | 883200        |\n",
            "| value_loss         | 9833.823      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00012357565  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 136            |\n",
            "| ep_reward_mean     | -325           |\n",
            "| explained_variance | 0.321          |\n",
            "| fps                | 543            |\n",
            "| n_updates          | 7200           |\n",
            "| policy_entropy     | 2.0861344      |\n",
            "| policy_loss        | -0.00016575877 |\n",
            "| serial_timesteps   | 921600         |\n",
            "| time_elapsed       | 1.75e+03       |\n",
            "| total_timesteps    | 921600         |\n",
            "| value_loss         | 5569.46        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.001114459  |\n",
            "| clipfrac           | 0.0078125    |\n",
            "| ep_len_mean        | 141          |\n",
            "| ep_reward_mean     | -218         |\n",
            "| explained_variance | 0.0324       |\n",
            "| fps                | 559          |\n",
            "| n_updates          | 7500         |\n",
            "| policy_entropy     | 2.4553263    |\n",
            "| policy_loss        | -0.010048855 |\n",
            "| serial_timesteps   | 960000       |\n",
            "| time_elapsed       | 1.82e+03     |\n",
            "| total_timesteps    | 960000       |\n",
            "| value_loss         | 22200.336    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00078538514 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 136           |\n",
            "| ep_reward_mean     | -122          |\n",
            "| explained_variance | -0.744        |\n",
            "| fps                | 501           |\n",
            "| n_updates          | 7800          |\n",
            "| policy_entropy     | 2.320632      |\n",
            "| policy_loss        | -0.0033421866 |\n",
            "| serial_timesteps   | 998400        |\n",
            "| time_elapsed       | 1.89e+03      |\n",
            "| total_timesteps    | 998400        |\n",
            "| value_loss         | 658.7837      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00031200092  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 144            |\n",
            "| ep_reward_mean     | -156           |\n",
            "| explained_variance | 0.914          |\n",
            "| fps                | 519            |\n",
            "| n_updates          | 8100           |\n",
            "| policy_entropy     | 2.8531277      |\n",
            "| policy_loss        | -0.00059974147 |\n",
            "| serial_timesteps   | 1036800        |\n",
            "| time_elapsed       | 1.96e+03       |\n",
            "| total_timesteps    | 1036800        |\n",
            "| value_loss         | 1131.7805      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 6.5268896e-06  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 140            |\n",
            "| ep_reward_mean     | 25.1           |\n",
            "| explained_variance | 0.492          |\n",
            "| fps                | 550            |\n",
            "| n_updates          | 8400           |\n",
            "| policy_entropy     | 2.4214358      |\n",
            "| policy_loss        | -0.00036671828 |\n",
            "| serial_timesteps   | 1075200        |\n",
            "| time_elapsed       | 2.04e+03       |\n",
            "| total_timesteps    | 1075200        |\n",
            "| value_loss         | 3901.1716      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0002901794  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 145           |\n",
            "| ep_reward_mean     | 0.8           |\n",
            "| explained_variance | 0.615         |\n",
            "| fps                | 510           |\n",
            "| n_updates          | 8700          |\n",
            "| policy_entropy     | 2.144363      |\n",
            "| policy_loss        | -0.0028717988 |\n",
            "| serial_timesteps   | 1113600       |\n",
            "| time_elapsed       | 2.11e+03      |\n",
            "| total_timesteps    | 1113600       |\n",
            "| value_loss         | 5462.5386     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.3173687e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 139           |\n",
            "| ep_reward_mean     | -33.8         |\n",
            "| explained_variance | -0.111        |\n",
            "| fps                | 579           |\n",
            "| n_updates          | 9000          |\n",
            "| policy_entropy     | 2.1623268     |\n",
            "| policy_loss        | 0.0006472751  |\n",
            "| serial_timesteps   | 1152000       |\n",
            "| time_elapsed       | 2.18e+03      |\n",
            "| total_timesteps    | 1152000       |\n",
            "| value_loss         | 7805.0557     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00027475212 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 144           |\n",
            "| ep_reward_mean     | 204           |\n",
            "| explained_variance | 0.427         |\n",
            "| fps                | 519           |\n",
            "| n_updates          | 9300          |\n",
            "| policy_entropy     | 1.992992      |\n",
            "| policy_loss        | -0.001096678  |\n",
            "| serial_timesteps   | 1190400       |\n",
            "| time_elapsed       | 2.25e+03      |\n",
            "| total_timesteps    | 1190400       |\n",
            "| value_loss         | 536.87213     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 3.644525e-06  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 144           |\n",
            "| ep_reward_mean     | 11.1          |\n",
            "| explained_variance | 0.103         |\n",
            "| fps                | 514           |\n",
            "| n_updates          | 9600          |\n",
            "| policy_entropy     | 1.9531403     |\n",
            "| policy_loss        | -0.0003863197 |\n",
            "| serial_timesteps   | 1228800       |\n",
            "| time_elapsed       | 2.32e+03      |\n",
            "| total_timesteps    | 1228800       |\n",
            "| value_loss         | 7581.32       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 1.8835773e-06  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 143            |\n",
            "| ep_reward_mean     | 32.1           |\n",
            "| explained_variance | 0.855          |\n",
            "| fps                | 544            |\n",
            "| n_updates          | 9900           |\n",
            "| policy_entropy     | 1.8315432      |\n",
            "| policy_loss        | -0.00011635781 |\n",
            "| serial_timesteps   | 1267200        |\n",
            "| time_elapsed       | 2.4e+03        |\n",
            "| total_timesteps    | 1267200        |\n",
            "| value_loss         | 508.9053       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0003690718  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 146           |\n",
            "| ep_reward_mean     | 181           |\n",
            "| explained_variance | 0.783         |\n",
            "| fps                | 565           |\n",
            "| n_updates          | 10200         |\n",
            "| policy_entropy     | 1.8936043     |\n",
            "| policy_loss        | 0.00033767847 |\n",
            "| serial_timesteps   | 1305600       |\n",
            "| time_elapsed       | 2.47e+03      |\n",
            "| total_timesteps    | 1305600       |\n",
            "| value_loss         | 148.95703     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.002002025  |\n",
            "| clipfrac           | 0.025390625  |\n",
            "| ep_len_mean        | 140          |\n",
            "| ep_reward_mean     | -55.7        |\n",
            "| explained_variance | 0.425        |\n",
            "| fps                | 548          |\n",
            "| n_updates          | 10500        |\n",
            "| policy_entropy     | 2.684748     |\n",
            "| policy_loss        | -0.011532115 |\n",
            "| serial_timesteps   | 1344000      |\n",
            "| time_elapsed       | 2.54e+03     |\n",
            "| total_timesteps    | 1344000      |\n",
            "| value_loss         | 7878.5386    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0011408525  |\n",
            "| clipfrac           | 0.001953125   |\n",
            "| ep_len_mean        | 140           |\n",
            "| ep_reward_mean     | -280          |\n",
            "| explained_variance | 0.401         |\n",
            "| fps                | 552           |\n",
            "| n_updates          | 10800         |\n",
            "| policy_entropy     | 2.5277472     |\n",
            "| policy_loss        | -0.0046586674 |\n",
            "| serial_timesteps   | 1382400       |\n",
            "| time_elapsed       | 2.61e+03      |\n",
            "| total_timesteps    | 1382400       |\n",
            "| value_loss         | 4678.995      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00030453893 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 143           |\n",
            "| ep_reward_mean     | -87.1         |\n",
            "| explained_variance | 0.48          |\n",
            "| fps                | 534           |\n",
            "| n_updates          | 11100         |\n",
            "| policy_entropy     | 2.5351875     |\n",
            "| policy_loss        | 0.0020486398  |\n",
            "| serial_timesteps   | 1420800       |\n",
            "| time_elapsed       | 2.68e+03      |\n",
            "| total_timesteps    | 1420800       |\n",
            "| value_loss         | 14900.334     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0010546923 |\n",
            "| clipfrac           | 0.0          |\n",
            "| ep_len_mean        | 143          |\n",
            "| ep_reward_mean     | -132         |\n",
            "| explained_variance | 0.32         |\n",
            "| fps                | 543          |\n",
            "| n_updates          | 11400        |\n",
            "| policy_entropy     | 2.3927946    |\n",
            "| policy_loss        | -0.009430047 |\n",
            "| serial_timesteps   | 1459200      |\n",
            "| time_elapsed       | 2.76e+03     |\n",
            "| total_timesteps    | 1459200      |\n",
            "| value_loss         | 5021.6675    |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.00019491087  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 142            |\n",
            "| ep_reward_mean     | 109            |\n",
            "| explained_variance | 0.408          |\n",
            "| fps                | 523            |\n",
            "| n_updates          | 11700          |\n",
            "| policy_entropy     | 1.8000958      |\n",
            "| policy_loss        | -0.00086834875 |\n",
            "| serial_timesteps   | 1497600        |\n",
            "| time_elapsed       | 2.83e+03       |\n",
            "| total_timesteps    | 1497600        |\n",
            "| value_loss         | 7463.005       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 5.9752824e-06  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 146            |\n",
            "| ep_reward_mean     | 165            |\n",
            "| explained_variance | 0.775          |\n",
            "| fps                | 563            |\n",
            "| n_updates          | 12000          |\n",
            "| policy_entropy     | 1.8661641      |\n",
            "| policy_loss        | -0.00031275162 |\n",
            "| serial_timesteps   | 1536000        |\n",
            "| time_elapsed       | 2.9e+03        |\n",
            "| total_timesteps    | 1536000        |\n",
            "| value_loss         | 254.30342      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0034501683 |\n",
            "| clipfrac           | 0.00390625   |\n",
            "| ep_len_mean        | 145          |\n",
            "| ep_reward_mean     | 231          |\n",
            "| explained_variance | 0.4          |\n",
            "| fps                | 498          |\n",
            "| n_updates          | 12300        |\n",
            "| policy_entropy     | 1.6142589    |\n",
            "| policy_loss        | 0.0037288547 |\n",
            "| serial_timesteps   | 1574400      |\n",
            "| time_elapsed       | 2.98e+03     |\n",
            "| total_timesteps    | 1574400      |\n",
            "| value_loss         | 13314.61     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.5208087e-06 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 143           |\n",
            "| ep_reward_mean     | 158           |\n",
            "| explained_variance | 0.49          |\n",
            "| fps                | 509           |\n",
            "| n_updates          | 12600         |\n",
            "| policy_entropy     | 1.7052729     |\n",
            "| policy_loss        | -0.0003636973 |\n",
            "| serial_timesteps   | 1612800       |\n",
            "| time_elapsed       | 3.05e+03      |\n",
            "| total_timesteps    | 1612800       |\n",
            "| value_loss         | 9623.7        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0005698077 |\n",
            "| clipfrac           | 0.0          |\n",
            "| ep_len_mean        | 145          |\n",
            "| ep_reward_mean     | 72.5         |\n",
            "| explained_variance | 0.695        |\n",
            "| fps                | 535          |\n",
            "| n_updates          | 12900        |\n",
            "| policy_entropy     | 2.0008261    |\n",
            "| policy_loss        | -0.003447846 |\n",
            "| serial_timesteps   | 1651200      |\n",
            "| time_elapsed       | 3.12e+03     |\n",
            "| total_timesteps    | 1651200      |\n",
            "| value_loss         | 704.09717    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00095034647 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 148           |\n",
            "| ep_reward_mean     | 177           |\n",
            "| explained_variance | 0.463         |\n",
            "| fps                | 558           |\n",
            "| n_updates          | 13200         |\n",
            "| policy_entropy     | 1.6804981     |\n",
            "| policy_loss        | 0.0066057704  |\n",
            "| serial_timesteps   | 1689600       |\n",
            "| time_elapsed       | 3.2e+03       |\n",
            "| total_timesteps    | 1689600       |\n",
            "| value_loss         | 1580.6978     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00069992046 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 144           |\n",
            "| ep_reward_mean     | 181           |\n",
            "| explained_variance | 0.47          |\n",
            "| fps                | 541           |\n",
            "| n_updates          | 13500         |\n",
            "| policy_entropy     | 1.8887358     |\n",
            "| policy_loss        | -0.0015095418 |\n",
            "| serial_timesteps   | 1728000       |\n",
            "| time_elapsed       | 3.27e+03      |\n",
            "| total_timesteps    | 1728000       |\n",
            "| value_loss         | 1087.3207     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 8.9175655e-06  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 143            |\n",
            "| ep_reward_mean     | 160            |\n",
            "| explained_variance | 0.385          |\n",
            "| fps                | 550            |\n",
            "| n_updates          | 13800          |\n",
            "| policy_entropy     | 1.8459654      |\n",
            "| policy_loss        | -0.00033630803 |\n",
            "| serial_timesteps   | 1766400        |\n",
            "| time_elapsed       | 3.34e+03       |\n",
            "| total_timesteps    | 1766400        |\n",
            "| value_loss         | 7741.3354      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.7062985e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 140           |\n",
            "| ep_reward_mean     | 176           |\n",
            "| explained_variance | 0.896         |\n",
            "| fps                | 537           |\n",
            "| n_updates          | 14100         |\n",
            "| policy_entropy     | 1.9718797     |\n",
            "| policy_loss        | -0.0013068791 |\n",
            "| serial_timesteps   | 1804800       |\n",
            "| time_elapsed       | 3.41e+03      |\n",
            "| total_timesteps    | 1804800       |\n",
            "| value_loss         | 277.65256     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 0.000122028345 |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 145            |\n",
            "| ep_reward_mean     | 224            |\n",
            "| explained_variance | 0.874          |\n",
            "| fps                | 502            |\n",
            "| n_updates          | 14400          |\n",
            "| policy_entropy     | 1.9074582      |\n",
            "| policy_loss        | -0.0010683639  |\n",
            "| serial_timesteps   | 1843200        |\n",
            "| time_elapsed       | 3.49e+03       |\n",
            "| total_timesteps    | 1843200        |\n",
            "| value_loss         | 227.94931      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.6506277e-06 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 142           |\n",
            "| ep_reward_mean     | 255           |\n",
            "| explained_variance | 0.895         |\n",
            "| fps                | 502           |\n",
            "| n_updates          | 14700         |\n",
            "| policy_entropy     | 1.7917583     |\n",
            "| policy_loss        | -2.476084e-05 |\n",
            "| serial_timesteps   | 1881600       |\n",
            "| time_elapsed       | 3.56e+03      |\n",
            "| total_timesteps    | 1881600       |\n",
            "| value_loss         | 288.98694     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0011241846 |\n",
            "| clipfrac           | 0.0          |\n",
            "| ep_len_mean        | 144          |\n",
            "| ep_reward_mean     | 172          |\n",
            "| explained_variance | 0.948        |\n",
            "| fps                | 564          |\n",
            "| n_updates          | 15000        |\n",
            "| policy_entropy     | 1.8005908    |\n",
            "| policy_loss        | 0.0019773412 |\n",
            "| serial_timesteps   | 1920000      |\n",
            "| time_elapsed       | 3.63e+03     |\n",
            "| total_timesteps    | 1920000      |\n",
            "| value_loss         | 171.92177    |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.552344e-06  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 142           |\n",
            "| ep_reward_mean     | 236           |\n",
            "| explained_variance | 0.887         |\n",
            "| fps                | 545           |\n",
            "| n_updates          | 15300         |\n",
            "| policy_entropy     | 1.577104      |\n",
            "| policy_loss        | -0.0001482591 |\n",
            "| serial_timesteps   | 1958400       |\n",
            "| time_elapsed       | 3.71e+03      |\n",
            "| total_timesteps    | 1958400       |\n",
            "| value_loss         | 156.32939     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00087902794 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 144           |\n",
            "| ep_reward_mean     | 241           |\n",
            "| explained_variance | 0.706         |\n",
            "| fps                | 508           |\n",
            "| n_updates          | 15600         |\n",
            "| policy_entropy     | 1.9383116     |\n",
            "| policy_loss        | 0.0023195536  |\n",
            "| serial_timesteps   | 1996800       |\n",
            "| time_elapsed       | 3.78e+03      |\n",
            "| total_timesteps    | 1996800       |\n",
            "| value_loss         | 477.22144     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.0024683625  |\n",
            "| clipfrac           | 0.0234375     |\n",
            "| ep_len_mean        | 146           |\n",
            "| ep_reward_mean     | 220           |\n",
            "| explained_variance | 0.199         |\n",
            "| fps                | 529           |\n",
            "| n_updates          | 15900         |\n",
            "| policy_entropy     | 1.662153      |\n",
            "| policy_loss        | -0.0042143976 |\n",
            "| serial_timesteps   | 2035200       |\n",
            "| time_elapsed       | 3.85e+03      |\n",
            "| total_timesteps    | 2035200       |\n",
            "| value_loss         | 9786.891      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 3.8611156e-06  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 141            |\n",
            "| ep_reward_mean     | 101            |\n",
            "| explained_variance | 0.501          |\n",
            "| fps                | 496            |\n",
            "| n_updates          | 16200          |\n",
            "| policy_entropy     | 1.634881       |\n",
            "| policy_loss        | -0.00032831682 |\n",
            "| serial_timesteps   | 2073600        |\n",
            "| time_elapsed       | 3.93e+03       |\n",
            "| total_timesteps    | 2073600        |\n",
            "| value_loss         | 658.8053       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 6.686055e-05  |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 144           |\n",
            "| ep_reward_mean     | 263           |\n",
            "| explained_variance | 0.515         |\n",
            "| fps                | 493           |\n",
            "| n_updates          | 16500         |\n",
            "| policy_entropy     | 1.6529605     |\n",
            "| policy_loss        | -0.0015056734 |\n",
            "| serial_timesteps   | 2112000       |\n",
            "| time_elapsed       | 4e+03         |\n",
            "| total_timesteps    | 2112000       |\n",
            "| value_loss         | 8306.466      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 4.9778764e-06 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 143           |\n",
            "| ep_reward_mean     | 292           |\n",
            "| explained_variance | 0.679         |\n",
            "| fps                | 557           |\n",
            "| n_updates          | 16800         |\n",
            "| policy_entropy     | 1.5978136     |\n",
            "| policy_loss        | -0.0006215356 |\n",
            "| serial_timesteps   | 2150400       |\n",
            "| time_elapsed       | 4.07e+03      |\n",
            "| total_timesteps    | 2150400       |\n",
            "| value_loss         | 393.12216     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00014412578 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 141           |\n",
            "| ep_reward_mean     | 172           |\n",
            "| explained_variance | 0.754         |\n",
            "| fps                | 544           |\n",
            "| n_updates          | 17100         |\n",
            "| policy_entropy     | 1.6231806     |\n",
            "| policy_loss        | 0.000464843   |\n",
            "| serial_timesteps   | 2188800       |\n",
            "| time_elapsed       | 4.14e+03      |\n",
            "| total_timesteps    | 2188800       |\n",
            "| value_loss         | 427.4315      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00026573235 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 146           |\n",
            "| ep_reward_mean     | 296           |\n",
            "| explained_variance | 0.592         |\n",
            "| fps                | 533           |\n",
            "| n_updates          | 17400         |\n",
            "| policy_entropy     | 1.768256      |\n",
            "| policy_loss        | -0.0030651586 |\n",
            "| serial_timesteps   | 2227200       |\n",
            "| time_elapsed       | 4.22e+03      |\n",
            "| total_timesteps    | 2227200       |\n",
            "| value_loss         | 8216.904      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.0917969e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 148           |\n",
            "| ep_reward_mean     | 286           |\n",
            "| explained_variance | 0.454         |\n",
            "| fps                | 557           |\n",
            "| n_updates          | 17700         |\n",
            "| policy_entropy     | 1.5593057     |\n",
            "| policy_loss        | -0.0013315966 |\n",
            "| serial_timesteps   | 2265600       |\n",
            "| time_elapsed       | 4.29e+03      |\n",
            "| total_timesteps    | 2265600       |\n",
            "| value_loss         | 676.2427      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 8.611066e-06   |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 144            |\n",
            "| ep_reward_mean     | 93.8           |\n",
            "| explained_variance | 0.946          |\n",
            "| fps                | 528            |\n",
            "| n_updates          | 18000          |\n",
            "| policy_entropy     | 1.8023918      |\n",
            "| policy_loss        | -0.00011052261 |\n",
            "| serial_timesteps   | 2304000        |\n",
            "| time_elapsed       | 4.36e+03       |\n",
            "| total_timesteps    | 2304000        |\n",
            "| value_loss         | 154.72833      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 3.3786043e-06 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 146           |\n",
            "| ep_reward_mean     | 183           |\n",
            "| explained_variance | 0.607         |\n",
            "| fps                | 566           |\n",
            "| n_updates          | 18300         |\n",
            "| policy_entropy     | 1.1925203     |\n",
            "| policy_loss        | -0.000637627  |\n",
            "| serial_timesteps   | 2342400       |\n",
            "| time_elapsed       | 4.44e+03      |\n",
            "| total_timesteps    | 2342400       |\n",
            "| value_loss         | 4894.1895     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 1.6564267e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 148           |\n",
            "| ep_reward_mean     | 257           |\n",
            "| explained_variance | 0.899         |\n",
            "| fps                | 475           |\n",
            "| n_updates          | 18600         |\n",
            "| policy_entropy     | 1.5483482     |\n",
            "| policy_loss        | -0.0011008597 |\n",
            "| serial_timesteps   | 2380800       |\n",
            "| time_elapsed       | 4.51e+03      |\n",
            "| total_timesteps    | 2380800       |\n",
            "| value_loss         | 480.99298     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 5.6816454e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 149           |\n",
            "| ep_reward_mean     | 259           |\n",
            "| explained_variance | 0.749         |\n",
            "| fps                | 490           |\n",
            "| n_updates          | 18900         |\n",
            "| policy_entropy     | 1.5094743     |\n",
            "| policy_loss        | -0.0012748874 |\n",
            "| serial_timesteps   | 2419200       |\n",
            "| time_elapsed       | 4.58e+03      |\n",
            "| total_timesteps    | 2419200       |\n",
            "| value_loss         | 643.29193     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00023454978 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 147           |\n",
            "| ep_reward_mean     | 205           |\n",
            "| explained_variance | 0.463         |\n",
            "| fps                | 554           |\n",
            "| n_updates          | 19200         |\n",
            "| policy_entropy     | 1.687564      |\n",
            "| policy_loss        | 0.0005411258  |\n",
            "| serial_timesteps   | 2457600       |\n",
            "| time_elapsed       | 4.65e+03      |\n",
            "| total_timesteps    | 2457600       |\n",
            "| value_loss         | 7102.195      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0012508079 |\n",
            "| clipfrac           | 0.013671875  |\n",
            "| ep_len_mean        | 150          |\n",
            "| ep_reward_mean     | 264          |\n",
            "| explained_variance | 0.757        |\n",
            "| fps                | 513          |\n",
            "| n_updates          | 19500        |\n",
            "| policy_entropy     | 1.7769458    |\n",
            "| policy_loss        | 0.0004065812 |\n",
            "| serial_timesteps   | 2496000      |\n",
            "| time_elapsed       | 4.73e+03     |\n",
            "| total_timesteps    | 2496000      |\n",
            "| value_loss         | 686.3221     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| approxkl           | 3.4964883e-06  |\n",
            "| clipfrac           | 0.0            |\n",
            "| ep_len_mean        | 149            |\n",
            "| ep_reward_mean     | 335            |\n",
            "| explained_variance | 0.895          |\n",
            "| fps                | 531            |\n",
            "| n_updates          | 19800          |\n",
            "| policy_entropy     | 1.3654202      |\n",
            "| policy_loss        | -0.00043877348 |\n",
            "| serial_timesteps   | 2534400        |\n",
            "| time_elapsed       | 4.8e+03        |\n",
            "| total_timesteps    | 2534400        |\n",
            "| value_loss         | 328.41476      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 2.9080837e-05 |\n",
            "| clipfrac           | 0.0           |\n",
            "| ep_len_mean        | 150           |\n",
            "| ep_reward_mean     | 353           |\n",
            "| explained_variance | 0.444         |\n",
            "| fps                | 547           |\n",
            "| n_updates          | 20100         |\n",
            "| policy_entropy     | 1.1464658     |\n",
            "| policy_loss        | -0.0018709252 |\n",
            "| serial_timesteps   | 2572800       |\n",
            "| time_elapsed       | 4.87e+03      |\n",
            "| total_timesteps    | 2572800       |\n",
            "| value_loss         | 1192.8594     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00016456374 |\n",
            "| clipfrac           | 0.001953125   |\n",
            "| ep_len_mean        | 148           |\n",
            "| ep_reward_mean     | 356           |\n",
            "| explained_variance | 0.758         |\n",
            "| fps                | 520           |\n",
            "| n_updates          | 20400         |\n",
            "| policy_entropy     | 1.1658255     |\n",
            "| policy_loss        | -0.0008956746 |\n",
            "| serial_timesteps   | 2611200       |\n",
            "| time_elapsed       | 4.95e+03      |\n",
            "| total_timesteps    | 2611200       |\n",
            "| value_loss         | 730.32434     |\n",
            "--------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2il_-d329yz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "909a4082-0e66-448a-c6c3-7003f9149aff"
      },
      "source": [
        "from stable_baselines import PPO2\n",
        "from stable_baselines.common.evaluation import evaluate_policy\n",
        "\n",
        "# Load saved model\n",
        "PPO_model = PPO2.load(\"dinerDash\")\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(PPO_model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "mean_reward:-35.80 +/- 130.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AhDY9hj1rht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testPPO2(env, obs):\n",
        "  from stable_baselines import PPO2\n",
        "\n",
        "  # Load saved model\n",
        "  PPO_model = PPO2.load(\"dinerDash\")\n",
        "\n",
        "  done = False\n",
        "  sum_rewards = 0\n",
        "  action_list = []\n",
        "\n",
        "  while not done:\n",
        "    action, _states = PPO_model.predict(obs)\n",
        "    action_list.append(action.item())\n",
        "    obs, rewards, done, info = env.step(action)\n",
        "    sum_rewards += rewards\n",
        "\n",
        "  return sum_rewards, action_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SolGR2yNPGCQ",
        "colab_type": "text"
      },
      "source": [
        "# Testing of Policies and Verification of Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWh2kSf9OGv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "import json\n",
        "from os import getcwd\n",
        "\n",
        "# Sample test\n",
        "def test():\n",
        "    # Initiliase environment\n",
        "    env = gym.make('diner_dash:DinerDash-v0').unwrapped\n",
        "    env.flash_sim = False\n",
        "\n",
        "    ############################ CHANGEABLE AREA ##############################\n",
        "    # Changeable parameters\n",
        "    numEpisodes = 100                             # num of test episodes\n",
        "    algos = [testPPO2]           # Add or remove algos (must have unique names)\n",
        "    saveJson = False                              # Whether to save actions_dict\n",
        "    group_members = \"john, mary, bryan\"           # String of group members\n",
        "    fileDirectory = \"./\"                          # Path of saved json file\n",
        "    fileName = \"submission.json\"                  # Name of json file\n",
        "\n",
        "    ### Replace the list of randomSeeds with that given for submission\n",
        "    # e.g. randomSeeds = [1, 2, 3]\n",
        "    randomSeeds = [randint(0, 1e8) for i in range(numEpisodes)]\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "    rewards_dict = {algo.__name__ : [] for algo in algos}\n",
        "    actions_dict = {algo.__name__ : [] for algo in algos}\n",
        "\n",
        "    # Test begins\n",
        "    for seed in randomSeeds:\n",
        "        # Sets random seed\n",
        "        env.seed(seed)\n",
        "        \n",
        "        # Resets the environment based on random seed\n",
        "        state = env.reset()\n",
        "\n",
        "        for algo in algos:\n",
        "            # create copy of environment for testing\n",
        "            t_env = env.env.duplicate()\n",
        "\n",
        "            # Given an environment and initial state\n",
        "            # Returns the sum of rewards for that episode and the actions list\n",
        "            rewards, actions = algo(t_env, state)\n",
        "\n",
        "            rewards_dict[algo.__name__].append(rewards)\n",
        "            actions_dict[algo.__name__].append(actions)\n",
        "\n",
        "    # Print average rewards from n episodes for each algo\n",
        "    avgReward_dict = {algo : int(sum(rewards)/len(rewards)) for algo, rewards in rewards_dict.items()}\n",
        "    print(f\"Average Rewards for each algo: {avgReward_dict}\")\n",
        "\n",
        "    # Print an action dict containing actions list for each random seed env for each algo\n",
        "    print(f\"Actions list for each env for each algo: {actions_dict}\")\n",
        "    \n",
        "    submission_dict = {\n",
        "        \"names\": group_members,\n",
        "        \"actionDict\": actions_dict}\n",
        "\n",
        "    if saveJson:\n",
        "      print(\"Saving Json file...\")\n",
        "      with open(fileDirectory + fileName, \"w\") as write_file:\n",
        "          json.dump(submission_dict, write_file)\n",
        "          print(f\"{fileName} was saved in {getcwd()}\")\n",
        "      \n",
        "      print(\"-\" * 100)\n",
        "      \n",
        "      print(f\"Verifying {fileName}...\")\n",
        "      print(f\"Group Members include: {submission_dict['names']}\")\n",
        "      print(f\"Names of Algos used: {list(submission_dict['actionDict'].keys())}\")\n",
        "      for val in submission_dict['actionDict'].values():\n",
        "        submissionEpisodes = len(val)\n",
        "        if submissionEpisodes != len(randomSeeds):\n",
        "          raise ValueError(\"Number of episodes in submission does not match the number of random seeds!\")\n",
        "      print(f\"Number of episodes(random seeds): {submissionEpisodes}\")\n",
        "      print(\"Number of episodes in submission matches the number of random seeds\")\n",
        "      print(\"Verification Complete! Please double check the verification results\")\n",
        "    \n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EwikBvPOstH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "41ae92fc-1e3c-40dd-975d-46e99e77e14f"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
            "Average Rewards for each algo: {'testPPO2': -43}\n",
            "Actions list for each env for each algo: {'testPPO2': [[14, 55, 52, 14, 2, 6, 13, 41, 43, 6, 34, 6, 48, 12, 6, 52, 42, 32, 44, 6, 6, 3, 52, 32, 6, 6, 2, 2, 6, 54, 4, 6, 20, 2, 14, 2, 2, 40, 2, 6, 22, 9, 6, 7, 2, 2, 7, 52, 52, 29, 54, 6, 2, 9, 52, 2, 52, 2, 54, 52, 52, 9, 2, 6, 9, 7, 9, 6, 9, 52, 4, 3, 29, 54, 2, 6, 9, 2, 9, 6, 2, 6, 2, 2, 9, 2, 2, 6, 2, 4, 4, 2, 9, 2, 7, 2, 2, 2, 52, 2, 2, 7, 9, 2, 4, 52, 2, 9, 9, 2, 7, 2, 4, 53, 2, 2, 54, 2, 2, 6, 2, 40], [19, 14, 0, 14, 32, 32, 45, 34, 14, 22, 51, 16, 54, 12, 6, 44, 12, 0, 17, 17, 11, 13, 8, 6, 7, 3, 54, 19, 41, 9, 12, 6, 52, 54, 7, 2, 17, 9, 2, 22, 6, 12, 2, 0, 0, 9, 6, 54, 6, 3, 54, 9, 3, 7, 54, 54, 2, 3, 54, 4, 2, 4, 46, 2, 9, 9, 2, 2, 2, 6, 2, 2, 9, 2, 3, 54, 2, 7, 2, 3, 12, 2, 29, 6, 2, 52, 3, 2, 2, 2, 2, 6, 3, 2, 52, 9, 7, 6, 6, 2, 2, 7, 2, 13, 2, 2, 52, 7, 2, 9, 7, 6, 6, 54, 2, 2, 6, 9, 4, 2, 7, 2, 2, 2, 2, 4, 7, 2, 6, 6, 9, 2, 52, 2, 9, 9, 2, 7, 9, 6, 17, 52, 2, 6, 6, 6, 52, 13, 6], [12, 14, 48, 16, 41, 9, 56, 6, 4, 6, 2, 6, 6, 7, 6, 6, 4, 22, 7, 6, 13, 52, 28, 9, 52, 6, 52, 7, 0, 2, 54, 13, 9, 6, 52, 2, 9, 2, 6, 2, 7, 12, 2, 2, 7, 6, 2, 2, 6, 54, 9, 54, 54, 6, 7, 7, 2, 9, 2, 2, 2, 3, 2, 52, 40, 2, 4, 2, 54, 2, 2, 2, 4, 17, 46, 2, 6, 40, 3, 9, 2, 7, 7, 9, 2, 2, 13, 6, 2, 6, 9, 6, 6, 6, 6, 3, 2, 7, 3, 40, 2, 7, 2, 2, 40, 4, 2, 2, 6, 9, 4, 9, 4], [37, 45, 6, 14, 49, 0, 0, 32, 24, 32, 8, 14, 41, 34, 55, 14, 8, 12, 14, 9, 6, 6, 6, 9, 4, 46, 12, 6, 2, 52, 6, 2, 52, 7, 54, 52, 3, 56, 7, 6, 28, 2, 9, 6, 29, 7, 40, 17, 2, 7, 3, 2, 2, 9, 6, 3, 0, 6, 6, 2, 6, 6, 7, 46, 9, 2, 6, 2, 54, 2, 2, 40, 6, 2, 7, 6, 2, 6, 28, 2, 52, 2, 2, 2, 9, 2, 2, 7, 52, 17, 7, 2, 2, 6, 2, 6, 9, 9, 2, 6, 2, 6, 9, 2, 54, 2, 54, 2, 7, 2, 2, 7, 9, 9, 17, 2, 2, 4, 2, 2, 3, 3, 2, 29, 7, 40, 6, 6, 6, 2, 6, 2, 7, 7, 2, 2, 2, 4, 6, 2, 6, 2, 2, 3, 2, 2, 2, 7], [14, 55, 16, 8, 8, 45, 6, 54, 6, 2, 3, 6, 6, 9, 52, 54, 2, 2, 13, 6, 9, 7, 9, 6, 52, 54, 34, 6, 6, 2, 2, 52, 2, 52, 2, 8, 20, 9, 2, 2, 2, 17, 6, 9, 2, 7, 9, 2, 44, 4, 7, 2, 2, 2, 2, 8, 52, 2, 2, 53, 2, 2, 9, 6, 2, 9, 54, 9, 2, 9, 6, 2, 2, 3, 9, 2, 2, 2, 6, 2, 2, 9, 4, 9, 40, 9, 7, 2, 9, 9, 7, 54, 17, 13, 52, 3, 9, 2, 6, 52, 54, 2, 9, 13, 6, 52, 6, 2, 3, 6, 2, 54, 2, 2, 6, 9, 2, 9, 9, 6, 7, 2, 6, 7, 6, 2, 4, 4, 2, 2, 2, 7, 2, 7, 2, 3, 2, 8, 29, 6, 2, 2, 9, 9, 2, 4, 2, 6, 54, 7, 2, 2, 56, 2, 3, 9, 9, 41, 2, 2, 6, 6, 52, 2, 2, 6, 9], [22, 14, 32, 48, 16, 22, 32, 54, 6, 40, 12, 13, 6, 6, 4, 2, 52, 54, 7, 4, 9, 54, 3, 7, 2, 32, 34, 2, 17, 2, 46, 7, 17, 2, 4, 9, 2, 9, 9, 7, 6, 17, 6, 6, 4, 2, 6, 40, 2, 12, 4, 2, 6, 52, 32, 2, 2, 9, 4, 6, 2, 6, 9, 2, 6, 2, 9, 54, 9, 9, 6, 6, 2, 6, 6, 2, 53, 29, 29, 46, 2, 52, 7, 6, 2, 6, 4, 9, 40, 52, 7, 1, 54, 6, 7, 54, 6, 13, 7, 2, 2, 2, 54, 9, 4, 6, 2, 6, 3, 6, 2, 6, 52, 9, 3, 6, 9, 7, 7], [31, 12, 14, 16, 7, 9, 41, 3, 54, 32, 41, 52, 14, 6, 7, 29, 7, 9, 6, 6, 17, 9, 48, 52, 9, 6, 54, 52, 6, 7, 40, 2, 2, 2, 7, 6, 2, 2, 8, 2, 2, 2, 2, 7, 4, 9, 56, 13, 2, 7, 54, 2, 2, 2, 2, 7, 29, 7, 2, 6, 2, 7, 2, 9, 2, 2, 9, 52, 6, 52, 29, 2, 52, 9, 46, 40, 2, 2, 6, 6, 9, 2, 7, 54, 6, 6, 8, 7, 6, 7, 6, 2, 2, 3, 2, 9, 2, 6, 2, 52, 3, 6, 2, 2, 9, 54, 9, 7, 2, 9, 9, 2, 7], [12, 36, 16, 12, 2, 2, 2, 48, 29, 8, 8, 54, 52, 6, 9, 7, 2, 7, 6, 6, 52, 2, 6, 4, 6, 4, 6, 54, 54, 0, 17, 54, 6, 2, 9, 7, 56, 3, 6, 9, 3, 7, 2, 6, 46, 2, 6, 22, 7, 2, 52, 2, 2, 2, 7, 2, 3, 7, 6, 6, 9, 9, 9, 2, 52, 17, 40, 4, 2, 6, 52, 2, 7, 2, 2, 2, 8, 2, 4, 2, 29, 2, 6, 3, 9, 46, 32, 2, 7, 2, 8, 6, 2, 7, 2, 6, 54, 9, 7, 52, 52, 13, 4, 7, 6, 2, 6, 7, 6, 52, 22, 9, 2, 6, 3, 9, 52, 2, 6, 9, 2, 2, 2, 2, 7, 2, 6, 4, 6, 2, 2, 3, 52, 2, 2, 54, 3, 2, 3], [31, 14, 2, 32, 32, 6, 52, 14, 14, 32, 8, 14, 32, 9, 54, 37, 32, 8, 36, 50, 6, 3, 34, 9, 9, 22, 8, 44, 6, 7, 8, 2, 29, 7, 6, 2, 6, 2, 2, 4, 2, 8, 4, 6, 2, 2, 2, 2, 2, 3, 9, 53, 2, 2, 6, 9, 7, 2, 2, 6, 2, 7, 7, 9, 9, 2, 6, 52, 9, 9, 9, 6, 3, 2, 2, 54, 40, 9, 2, 2, 7, 2, 2, 3, 2, 9, 2, 6, 9, 2, 56, 2, 56, 9, 9, 9, 2, 40, 2, 52, 12, 2, 9, 4, 46, 2, 6, 2, 6, 6, 9, 6, 3, 9, 9, 6, 6, 3, 9, 4, 2, 6, 9, 6, 2, 6, 6, 9, 54, 2, 7, 6, 0, 2, 9, 41, 54, 20], [24, 8, 14, 16, 52, 14, 7, 6, 6, 0, 34, 32, 13, 6, 6, 52, 4, 2, 51, 7, 6, 6, 6, 55, 8, 6, 6, 6, 52, 2, 2, 6, 52, 52, 4, 9, 6, 52, 2, 2, 0, 44, 54, 2, 4, 2, 2, 7, 2, 2, 56, 2, 29, 6, 6, 6, 6, 6, 40, 9, 2, 2, 2, 9, 9, 52, 2, 6, 4, 6, 3, 7, 3, 52, 13, 2, 2, 6, 2, 2, 2, 17, 9, 2, 8, 7, 7, 6, 0, 7, 2, 2, 6, 7, 2, 2, 7, 2, 7, 2, 7, 4, 2, 2, 2, 6, 6, 4, 41, 54, 54, 2, 2, 2, 2, 2, 2, 2, 52, 3, 2, 6, 52, 2, 4, 6, 53, 54, 2, 6, 6]]}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}